{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recommendation_News.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ybdrq2netC",
        "colab_type": "code",
        "outputId": "cc93ffb0-ca0e-4a1a-eaa3-80f21d3eeedd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install pymongo\n",
        "!pip install plotly"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (3.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly) (1.12.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly) (1.3.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc9828g4nk1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "# Below libraries are for text processing using NLTK\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Below libraries are for feature representation using sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Below libraries are for similarity matrices using sklearn\n",
        "from sklearn.metrics.pairwise import cosine_similarity  \n",
        "from sklearn.metrics import pairwise_distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04JIuamMnoD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pymongo import MongoClient\n",
        "#Step 1: Connect to MongoDB - Note: Change connection string as needed\n",
        "client = MongoClient(port=27017)\n",
        "db=client.newsdb\n",
        "collection = db.new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY5pbVdSnqn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing requests package \n",
        "import requests\t \n",
        "import json\n",
        "def News(): \n",
        "\t\n",
        "\t# BBC news api \n",
        "  main_url=[\"http://newsapi.org/v2/top-headlines?country=us&category=sports&apiKey=0c9ce5c54abc446bbbe5d20e5e6f2fe9\",\"http://newsapi.org/v2/top-headlines?country=us&category=business&apiKey=0c9ce5c54abc446bbbe5d20e5e6f2fe9\",\"http://newsapi.org/v2/top-headlines?country=us&category=politics&apiKey=0c9ce5c54abc446bbbe5d20e5e6f2fe9\",\"http://newsapi.org/v2/top-headlines?country=us&category=technology&apiKey=0c9ce5c54abc446bbbe5d20e5e6f2fe9\",\"http://newsapi.org/v2/top-headlines?country=us&category=health&apiKey=0c9ce5c54abc446bbbe5d20e5e6f2fe9\"]\n",
        "  category=['sports','business','politics','technology','health']\n",
        "  for k in range(len(main_url)):\n",
        "\t# fetching data in json format \n",
        "    open_page = requests.get(main_url[k]).json() \n",
        "    article = open_page[\"articles\"]\n",
        "    for i in article:\n",
        "        i.update({'category':category[k]})\n",
        "        print(i)\n",
        "    collection.insert_many(article)\n",
        "\n",
        "\n",
        "# Driver Code \n",
        "if __name__ == '__main__': \n",
        "\tNews() \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTCv775LnttP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading the database into a pandas dataframe\n",
        "import pandas as pd\n",
        "news_articles = pd.DataFrame.from_records(collection.find())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbyIgGPOnwHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_articles.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_9abq7Ynyu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing articles with short headlines\n",
        "news_articles = news_articles[news_articles['title'].apply(lambda x: len(x.split())>5)]\n",
        "print(\"Total number of articles after removal of headlines with short title:\", news_articles.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONZ3DO8_n1fU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing duplicate articles\n",
        "news_articles.sort_values('title',inplace=True, ascending=False)\n",
        "duplicated_articles_series = news_articles.duplicated('title', keep = False)\n",
        "news_articles = news_articles[~duplicated_articles_series]\n",
        "print(\"Total number of articles after removing duplicates:\", news_articles.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8x2iUYGn38o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Finding number of null values in each attribute\n",
        "news_articles.isna().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srKSngxnn6Jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Total number of articles : \", news_articles.shape[0])\n",
        "print(\"Total number of authors : \", news_articles[\"author\"].nunique())\n",
        "print(\"Total number of unqiue categories : \", news_articles[\"category\"].nunique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r98DqD7rn81V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = go.Figure([go.Bar(x=news_articles[\"category\"].value_counts().index, y=news_articles[\"category\"].value_counts().values)])\n",
        "fig['layout'].update(title={\"text\" : 'Distribution of articles category-wise','y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'}, xaxis_title=\"Category name\",yaxis_title=\"Number of articles\")\n",
        "fig.update_layout(width=800,height=700)\n",
        "fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70W6yGnEn-8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Downloading the nltk english stopwords package for preprocessing\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uMPhqlloBxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_articles_temp = news_articles.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUg_BBWXEnjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing stopwords from headlines\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))   \n",
        "cnt=0\n",
        "for i in range(len(news_articles_temp[\"title\"])):\n",
        "    print(i)\n",
        "    try:\n",
        "        string = \"\"\n",
        "        word_tokens = word_tokenize(news_articles_temp[\"title\"][i])\n",
        "        print(word_tokens)\n",
        "        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "        filtered_sentence = [] \n",
        "\n",
        "        for w in word_tokens: \n",
        "            if w not in stop_words: \n",
        "                filtered_sentence.append(w)\n",
        "    except KeyError as ke:\n",
        "        cnt+=1\n",
        "        print('Key Not Found', ke)\n",
        "\n",
        "    for word in filtered_sentence:\n",
        "        word = (\"\".join(e for e in word if e.isalnum()))\n",
        "        word = word.lower()\n",
        "        if not word in stop_words:\n",
        "            string += word + \" \"  \n",
        "        if(i%10==0):\n",
        "            print(i)  # To track number of records processed\n",
        "    #news_articles_temp.at[i,\"title\"] = string.strip()\n",
        "print(cnt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_AekifCEvir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBw1rjdZExrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lemmatizing the headlines\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))   \n",
        "cnt=0\n",
        "for i in range(len(news_articles_temp[\"title\"])):\n",
        "    print(i)\n",
        "    try:\n",
        "        string = \"\"\n",
        "        word_tokens = word_tokenize(news_articles_temp[\"title\"][i])\n",
        "        print(word_tokens)\n",
        "        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "        filtered_sentence = [] \n",
        "\n",
        "        for w in word_tokens: \n",
        "            if w not in stop_words: \n",
        "                filtered_sentence.append(w)\n",
        "    except KeyError as ke:\n",
        "        cnt+=1\n",
        "        print('Key Not Found in Employee Dictionary:', ke)\n",
        "\n",
        "    for w in filtered_sentence:\n",
        "        string += lemmatizer.lemmatize(w,pos = \"v\") + \" \"\n",
        "    news_articles_temp.at[i, \"title\"] = string.strip()\n",
        "    if(i%10==0):\n",
        "        print(i)     # To track number of records processed\n",
        "    #news_articles_temp.at[i,\"title\"] = string.strip()\n",
        "print(cnt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVtOf_ojE0sw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headline_vectorizer = CountVectorizer()\n",
        "headline_features   = headline_vectorizer.fit_transform(news_articles_temp['title'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVXscvAIE48P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "headline_features.get_shape()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dng1mYiCE7Uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1) # To display a very long headline completely"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjuroaRPE7zU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To display results similar to the queried article's headline based on CountVectorizer\n",
        "def bag_of_words_based_model(row_index, num_similar_items):\n",
        "    couple_dist = pairwise_distances(headline_features,headline_features[row_index])\n",
        "    indices = np.argsort(couple_dist.ravel())[0:num_similar_items]\n",
        "    df = pd.DataFrame({'publish_date': news_articles['publishedAt'][indices].values,\n",
        "               'headline':news_articles['title'][indices].values,\n",
        "                'Euclidean similarity with the queried article': couple_dist[indices].ravel()})\n",
        "    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n",
        "    print('headline : ',news_articles['title'][indices[0]])\n",
        "    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n",
        "    #return df.iloc[1:,1]\n",
        "    return df.iloc[1:,]\n",
        "\n",
        "bag_of_words_based_model(15, 11)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIVWYyD2E-Ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To display results similar to the queried article's headline based on TfidfVectorizer\n",
        "tfidf_headline_vectorizer = TfidfVectorizer(min_df = 0)\n",
        "tfidf_headline_features = tfidf_headline_vectorizer.fit_transform(news_articles_temp['title'])\n",
        "def tfidf_based_model(row_index, num_similar_items):\n",
        "    couple_dist = pairwise_distances(tfidf_headline_features,tfidf_headline_features[row_index])\n",
        "    indices = np.argsort(couple_dist.ravel())[0:num_similar_items]\n",
        "    df = pd.DataFrame({'publish_date': news_articles['publishedAt'][indices].values,\n",
        "               'headline':news_articles['title'][indices].values,\n",
        "                'Euclidean similarity with the queried article': couple_dist[indices].ravel()})\n",
        "    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n",
        "    print('headline : ',news_articles['title'][indices[0]])\n",
        "    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n",
        "    df=df.dropna()\n",
        "    print(len(df))\n",
        "    #return df.iloc[1:,1]\n",
        "    return df\n",
        "tfidf_based_model(15, 11)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FMz1nwqFCjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install -U gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84e9-rrvFFa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Gensim is NLP package having Google's pretrained Word2vec model\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim import models\n",
        "import pickle\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6vSkQoYkhgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "loaded_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttHo8X4qkiez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary = loaded_model.vocab.keys()\n",
        "w2v_headline = []\n",
        "for i in news_articles_temp['title']:\n",
        "    w2Vec_word = np.zeros(300, dtype=\"float32\")\n",
        "    for word in i.split():\n",
        "        if word in vocabulary:\n",
        "            w2Vec_word = np.add(w2Vec_word, loaded_model[word])\n",
        "    w2Vec_word = np.divide(w2Vec_word, len(i.split()))\n",
        "    w2v_headline.append(w2Vec_word)\n",
        "w2v_headline = np.array(w2v_headline)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyznx_szkkZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Recommendation based on similar headlines\n",
        "def avg_w2v_based_model(row_index, num_similar_items):\n",
        "    couple_dist = pairwise_distances(w2v_headline, w2v_headline[row_index].reshape(1,-1))\n",
        "    indices = np.argsort(couple_dist.ravel())[0:num_similar_items]\n",
        "    df = pd.DataFrame({'publish_date': news_articles['publishedAt'][indices].values,\n",
        "               'headline':news_articles['title'][indices].values,\n",
        "                'Euclidean similarity with the queried article': couple_dist[indices].ravel()})\n",
        "    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n",
        "    print('headline : ',news_articles['title'][indices[0]])\n",
        "    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n",
        "    #return df.iloc[1:,1]\n",
        "    return df.iloc[1:,]\n",
        "\n",
        "avg_w2v_based_model(90, 11)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHrbzjo9koQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0elI-uqkpFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using OneHotEncoding on the catoegory feature\n",
        "news_articles_temp=news_articles_temp.dropna()\n",
        "category_onehot_encoded = OneHotEncoder().fit_transform(np.array(news_articles_temp[\"category\"]).reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8w9uicQkrNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Recommendation based on similar headlines and category\n",
        "def avg_w2v_with_category(row_index, num_similar_items, w1,w2): \n",
        "    w2v_dist  = pairwise_distances(w2v_headline, w2v_headline[row_index].reshape(1,-1))\n",
        "    \n",
        "    category_dist = pairwise_distances(category_onehot_encoded, category_onehot_encoded[row_index]) + 1\n",
        "\n",
        "    x=(w1 * w2v_dist +  w2 * category_dist)\n",
        "    weighted_couple_dist   = x/float(w1 + w2)\n",
        "    \n",
        "    indices = np.argsort(weighted_couple_dist.flatten())[0:num_similar_items].tolist()\n",
        "    df = pd.DataFrame({'publish_date': news_articles['publishedAt'][indices].values,\n",
        "               'headline':news_articles['title'][indices].values,\n",
        "                'Weighted Euclidean similarity with the queried article': weighted_couple_dist[indices].ravel(),\n",
        "                'Word2Vec based Euclidean similarity': w2v_dist[indices].ravel(),\n",
        "                 'Category based Euclidean similarity': category_dist[indices].ravel(),\n",
        "                 'Categoty': news_articles['category'][indices].values})\n",
        "    \n",
        "    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n",
        "    print('headline : ',news_articles['title'][indices[0]])\n",
        "    print('Categoty : ', news_articles['category'][indices[0]])\n",
        "    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n",
        "    #return df.iloc[1:,[1,5]]\n",
        "    return df.iloc[1:, ]\n",
        "\n",
        "avg_w2v_with_category(50,10,0.1,0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wty7ZA9wnrMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Performing OneHotEncoding on the authors\n",
        "authors_onehot_encoded = OneHotEncoder().fit_transform(np.array(news_articles_temp[\"author\"]).reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2f3uaJint6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Recommendation based on similar headlines,category and authors\n",
        "def avg_w2v_with_category_and_authors(row_index, num_similar_items, w1,w2,w3): \n",
        "    w2v_dist  = pairwise_distances(w2v_headline, w2v_headline[row_index].reshape(1,-1))\n",
        "    category_dist = pairwise_distances(category_onehot_encoded, category_onehot_encoded[row_index]) + 1\n",
        "    authors_dist = pairwise_distances(authors_onehot_encoded, authors_onehot_encoded[row_index]) + 1\n",
        "    weighted_couple_dist   = (w1 * w2v_dist +  w2 * category_dist + w3 * authors_dist)/float(w1 + w2 + w3)\n",
        "    indices = np.argsort(weighted_couple_dist.flatten())[0:num_similar_items].tolist()\n",
        "    df = pd.DataFrame({'publish_date': news_articles['publishedAt'][indices].values,\n",
        "                'headline':news_articles['title'][indices].values,\n",
        "                'Weighted Euclidean similarity with the queried article': weighted_couple_dist[indices].ravel(),\n",
        "                'Word2Vec based Euclidean similarity': w2v_dist[indices].ravel(),\n",
        "                'Category based Euclidean similarity': category_dist[indices].ravel(),\n",
        "                'Authors based Euclidean similarity': authors_dist[indices].ravel(),       \n",
        "                'Categoty': news_articles['category'][indices].values,\n",
        "                'Authors': news_articles['author'][indices].values})\n",
        "    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n",
        "    print('headline : ',news_articles['title'][indices[0]])\n",
        "    print('Categoty : ', news_articles['category'][indices[0]])\n",
        "    print('Authors : ', news_articles['author'][indices[0]])\n",
        "    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n",
        "    #return df.iloc[1:,[1,6,7]]\n",
        "    return df.iloc[1:, ]\n",
        "    \n",
        "avg_w2v_with_category_and_authors(50,10,0.1,0.1,1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN2V0pennwOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Performing OneHotEncoding on the published time\n",
        "publishingday_onehot_encoded = OneHotEncoder().fit_transform(np.array(news_articles_temp[\"publishedAt\"]).reshape(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6LmWag_uefe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Recommendation based on similar headlines,category,author and publishing time\n",
        "def avg_w2v_with_category_authors_and_publshing_day(row_index, num_similar_items, w1,w2,w3,w4): \n",
        "    w2v_dist  = pairwise_distances(w2v_headline, w2v_headline[row_index].reshape(1,-1))\n",
        "    category_dist = pairwise_distances(category_onehot_encoded, category_onehot_encoded[row_index]) + 1\n",
        "    authors_dist = pairwise_distances(authors_onehot_encoded, authors_onehot_encoded[row_index]) + 1\n",
        "    publishingday_dist = pairwise_distances(publishingday_onehot_encoded, publishingday_onehot_encoded[row_index]) + 1\n",
        "    weighted_couple_dist   = (w1 * w2v_dist +  w2 * category_dist + w3 * authors_dist + w4 * publishingday_dist)/float(w1 + w2 + w3 + w4)\n",
        "    indices = np.argsort(weighted_couple_dist.flatten())[0:num_similar_items].tolist()\n",
        "    df = pd.DataFrame({'publish_date': news_articles['publishedAt'][indices].values,\n",
        "                'headline_text':news_articles['title'][indices].values,\n",
        "                'Weighted Euclidean similarity with the queried article': weighted_couple_dist[indices].ravel(),\n",
        "                'Word2Vec based Euclidean similarity': w2v_dist[indices].ravel(),\n",
        "                'Category based Euclidean similarity': category_dist[indices].ravel(),\n",
        "                'Authors based Euclidean similarity': authors_dist[indices].ravel(),   \n",
        "                'Publishing day based Euclidean similarity': publishingday_dist[indices].ravel(), \n",
        "                'Categoty': news_articles['category'][indices].values,\n",
        "                'Authors': news_articles['author'][indices].values,\n",
        "                'Day and month': news_articles['publishedAt'][indices].values})\n",
        "    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n",
        "    print('headline : ',news_articles['title'][indices[0]])\n",
        "    print('Categoty : ', news_articles['category'][indices[0]])\n",
        "    print('Authors : ', news_articles['author'][indices[0]])\n",
        "    print('Day and month : ', news_articles['publishedAt'][indices[0]])\n",
        "    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n",
        "    #return df.iloc[1:,[1,7,8,9]]\n",
        "    return df.iloc[1:, ]\n",
        "\n",
        "\n",
        "avg_w2v_with_category_authors_and_publshing_day(50,10,0.1,0.1,0.1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}